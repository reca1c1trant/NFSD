# Stability AI æ‰©æ•£æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæ•´è§£æ

## ç›®å½•
1. [è®­ç»ƒæµç¨‹æ¦‚è§ˆ](#1-è®­ç»ƒæµç¨‹æ¦‚è§ˆ)
2. [æ•°å­¦åŸºç¡€ä¸å…¬å¼](#2-æ•°å­¦åŸºç¡€ä¸å…¬å¼)
3. [å®Œæ•´è®­ç»ƒç®—æ³•](#3-å®Œæ•´è®­ç»ƒç®—æ³•)
4. [ä»£ç å®ç°è¯¦è§£](#4-ä»£ç å®ç°è¯¦è§£)
5. [å¯æ›¿æ¢ç‚¹åˆ†æ](#5-å¯æ›¿æ¢ç‚¹åˆ†æ)

---

## 1. è®­ç»ƒæµç¨‹æ¦‚è§ˆ

### 1.1 æ•´ä½“æ¶æ„

```
æ•°æ®åŠ è½½
    â†“
VAE ç¼–ç  (å›¾åƒ â†’ æ½œåœ¨ç©ºé—´)
    â†“
æ·»åŠ å™ªå£° (å‰å‘æ‰©æ•£)
    â†“
æ¨¡å‹é¢„æµ‹ (UNet + Denoiser)
    â†“
è®¡ç®—æŸå¤± (ä¸ç›®æ ‡å¯¹æ¯”)
    â†“
åå‘ä¼ æ’­
    â†“
æ›´æ–°æƒé‡
```

### 1.2 è®­ç»ƒå¾ªç¯

**ä»£ç ä½ç½®ï¼š** `sgm/models/diffusion.py:165-187`

```python
def training_step(self, batch, batch_idx):
    # 1. è·å–è¾“å…¥å¹¶ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
    loss, loss_dict = self.shared_step(batch)

    # 2. è®°å½•æ—¥å¿—
    self.log_dict(loss_dict, ...)

    # 3. è¿”å›æŸå¤±è¿›è¡Œåå‘ä¼ æ’­
    return loss
```

---

## 2. æ•°å­¦åŸºç¡€ä¸å…¬å¼

### 2.1 å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆå›ºå®šï¼‰

**å®šä¹‰ï¼š**
$$\mathbf{x}_t = \mathbf{x}_0 + \sigma_t \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$$

å…¶ä¸­ï¼š
- $\mathbf{x}_0$ï¼šå¹²å‡€æ•°æ®ï¼ˆç›®æ ‡ï¼‰
- $\mathbf{x}_t$ï¼šå™ªå£°æ•°æ®ï¼ˆè¾“å…¥ï¼‰
- $\sigma_t$ï¼šå™ªå£°çº§åˆ«
- $\boldsymbol{\epsilon}$ï¼šæ ‡å‡†é«˜æ–¯å™ªå£°

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/loss.py:42-46, 84`

```python
noise = torch.randn_like(input)              # Îµ ~ N(0, I)
sigmas_bc = append_dims(sigmas, input.ndim)  # Ïƒ_t
noised_input = input + noise * sigmas_bc     # x_t = x_0 + Ïƒ_t * Îµ
```

### 2.2 æ¨¡å‹è¾“å‡ºï¼ˆå½“å‰å®ç°ï¼‰

**å»å™ªæ ·æœ¬é¢„æµ‹ï¼š**
$$\hat{\mathbf{x}}_\theta(\mathbf{x}_t, \sigma_t, \mathbf{c}) = D_\theta(\mathbf{x}_t, \sigma_t, \mathbf{c})$$

å…¶ä¸­ï¼š
- $D_\theta$ï¼šå»å™ªå™¨ï¼ˆDenoiserï¼‰
- $\mathbf{c}$ï¼šæ¡ä»¶ï¼ˆæ–‡æœ¬ã€ç±»åˆ«ç­‰ï¼‰
- $\hat{\mathbf{x}}_\theta$ï¼šé¢„æµ‹çš„å¹²å‡€æ•°æ®

**å®é™…å®ç°ï¼ˆå¸¦é¢„æ¡ä»¶ï¼‰ï¼š**
$$\boxed{\hat{\mathbf{x}}_\theta = c_\text{skip}(\sigma_t) \cdot \mathbf{x}_t + c_\text{out}(\sigma_t) \cdot F_\theta(c_\text{in}(\sigma_t) \cdot \mathbf{x}_t, c_\text{noise}(\sigma_t), \mathbf{c})}$$

å…¶ä¸­ï¼š
- $F_\theta$ï¼šåŸå§‹ç½‘ç»œï¼ˆUNetï¼‰
- $c_\text{skip}, c_\text{out}, c_\text{in}, c_\text{noise}$ï¼šé¢„æ¡ä»¶ç³»æ•°ï¼ˆä¾èµ– $\sigma_t$ï¼‰

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser.py:23-39`

```python
def forward(self, network, input, sigma, cond, **kwargs):
    sigma = append_dims(sigma, input.ndim)
    c_skip, c_out, c_in, c_noise = self.scaling(sigma)
    return (
        network(input * c_in, c_noise, cond, **kwargs) * c_out
        + input * c_skip
    )
```

### 2.3 è®­ç»ƒç›®æ ‡ï¼ˆæŸå¤±å‡½æ•°ï¼‰

**åŠ æƒ MSE æŸå¤±ï¼š**
$$\boxed{\mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}, \sigma_t} \left[ w(\sigma_t) \left\| \hat{\mathbf{x}}_\theta(\mathbf{x}_t, \sigma_t, \mathbf{c}) - \mathbf{x}_0 \right\|_2^2 \right]}$$

å…¶ä¸­ï¼š
- $w(\sigma_t)$ï¼šå™ªå£°çº§åˆ«ç›¸å…³çš„æƒé‡å‡½æ•°
- $\|\cdot\|_2^2$ï¼šL2 èŒƒæ•°å¹³æ–¹ï¼ˆMSEï¼‰

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/loss.py:59-90, 92-96`

```python
# é‡‡æ ·å™ªå£°çº§åˆ«
sigmas = self.sigma_sampler(input.shape[0])

# æ·»åŠ å™ªå£°
noised_input = input + noise * sigmas_bc

# æ¨¡å‹é¢„æµ‹
model_output = denoiser(network, noised_input, sigmas, cond)

# è®¡ç®—æƒé‡
w = self.loss_weighting(sigmas)

# æŸå¤±
loss = torch.mean(w * (model_output - input) ** 2)
```

### 2.4 ä¸åŒçš„é¢„æ¡ä»¶æ–¹æ¡ˆ

#### A. EpsScalingï¼ˆDDPM é£æ ¼ï¼Œé¢„æµ‹å™ªå£°ï¼‰

**é¢„æ¡ä»¶ç³»æ•°ï¼š**
$$\begin{aligned}
c_\text{skip} &= 1 \\
c_\text{out} &= -\sigma_t \\
c_\text{in} &= \frac{1}{\sqrt{\sigma_t^2 + 1}} \\
c_\text{noise} &= \sigma_t
\end{aligned}$$

**å»å™ªæ ·æœ¬å…¬å¼ï¼š**
$$\hat{\mathbf{x}}_\theta = \mathbf{x}_t - \sigma_t \cdot F_\theta\left(\frac{\mathbf{x}_t}{\sqrt{\sigma_t^2 + 1}}, \sigma_t, \mathbf{c}\right)$$

**è§£é‡Šï¼š** $F_\theta$ é¢„æµ‹å™ªå£° $\boldsymbol{\epsilon}$

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_scaling.py:29-37`

#### B. VScalingï¼ˆé€Ÿåº¦é¢„æµ‹ï¼‰

**é¢„æ¡ä»¶ç³»æ•°ï¼š**
$$\begin{aligned}
c_\text{skip} &= \frac{1}{\sigma_t^2 + 1} \\
c_\text{out} &= \frac{-\sigma_t}{\sqrt{\sigma_t^2 + 1}} \\
c_\text{in} &= \frac{1}{\sqrt{\sigma_t^2 + 1}} \\
c_\text{noise} &= \sigma_t
\end{aligned}$$

**å»å™ªæ ·æœ¬å…¬å¼ï¼š**
$$\hat{\mathbf{x}}_\theta = \frac{1}{\sigma_t^2 + 1} \mathbf{x}_t - \frac{\sigma_t}{\sqrt{\sigma_t^2 + 1}} F_\theta\left(\frac{\mathbf{x}_t}{\sqrt{\sigma_t^2 + 1}}, \sigma_t, \mathbf{c}\right)$$

**è§£é‡Šï¼š** $F_\theta$ é¢„æµ‹é€Ÿåº¦ï¼ˆæ•°æ®å’Œå™ªå£°çš„ç»„åˆï¼‰

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_scaling.py:40-48`

#### C. EDMScalingï¼ˆKarras ç­‰ï¼Œ2022ï¼‰

**é¢„æ¡ä»¶ç³»æ•°ï¼š**
$$\begin{aligned}
c_\text{skip} &= \frac{\sigma_\text{data}^2}{\sigma_t^2 + \sigma_\text{data}^2} \\
c_\text{out} &= \frac{\sigma_t \sigma_\text{data}}{\sqrt{\sigma_t^2 + \sigma_\text{data}^2}} \\
c_\text{in} &= \frac{1}{\sqrt{\sigma_t^2 + \sigma_\text{data}^2}} \\
c_\text{noise} &= \frac{1}{4} \ln \sigma_t
\end{aligned}$$

å…¶ä¸­ $\sigma_\text{data}$ é€šå¸¸å– 0.5ã€‚

**å»å™ªæ ·æœ¬å…¬å¼ï¼š**
$$\hat{\mathbf{x}}_\theta = \frac{\sigma_\text{data}^2}{\sigma_t^2 + \sigma_\text{data}^2} \mathbf{x}_t + \frac{\sigma_t \sigma_\text{data}}{\sqrt{\sigma_t^2 + \sigma_\text{data}^2}} F_\theta\left(\frac{\mathbf{x}_t}{\sqrt{\sigma_t^2 + \sigma_\text{data}^2}}, \frac{\ln \sigma_t}{4}, \mathbf{c}\right)$$

**è§£é‡Šï¼š** $F_\theta$ é¢„æµ‹ç¼©æ”¾æ®‹å·®

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_scaling.py:15-26`

### 2.5 å™ªå£°çº§åˆ«é‡‡æ ·

**EDM é‡‡æ ·ï¼ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰ï¼š**
$$\log \sigma_t \sim \mathcal{N}(p_\text{mean}, p_\text{std}^2)$$

$$\sigma_t = \exp(\log \sigma_t)$$

**é»˜è®¤å‚æ•°ï¼š** $p_\text{mean} = -1.2$ï¼Œ$p_\text{std} = 1.2$

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/sigma_sampling.py:6-13`

```python
class EDMSampling:
    def __init__(self, p_mean=-1.2, p_std=1.2):
        self.p_mean = p_mean
        self.p_std = p_std

    def __call__(self, n_samples, rand=None):
        log_sigma = self.p_mean + self.p_std * torch.randn((n_samples,))
        return log_sigma.exp()
```

### 2.6 æŸå¤±æƒé‡

#### A. EDM æƒé‡

$$w(\sigma_t) = \frac{\sigma_t^2 + \sigma_\text{data}^2}{(\sigma_t \cdot \sigma_\text{data})^2}$$

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_weighting.py:9-14`

#### B. Eps æƒé‡

$$w(\sigma_t) = \sigma_t^{-2}$$

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_weighting.py:22-24`

#### C. Unit æƒé‡

$$w(\sigma_t) = 1$$

**ä»£ç ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_weighting.py:4-6`

---

## 3. å®Œæ•´è®­ç»ƒç®—æ³•

### 3.1 ä¼ªä»£ç 

```
ç®—æ³•ï¼šæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼ˆStability AI å®ç°ï¼‰

è¾“å…¥ï¼š
    - è®­ç»ƒæ•°æ®é›† D = {x_0^(i)}
    - æ¨¡å‹ F_Î¸ï¼ˆUNetï¼‰
    - å»å™ªå™¨ç¼©æ”¾ scaling(Ïƒ)
    - æ¡ä»¶å™¨ conditioner
    - å™ªå£°é‡‡æ ·å™¨ sigma_sampler
    - æŸå¤±æƒé‡ loss_weighting
    - ç¬¬ä¸€é˜¶æ®µæ¨¡å‹ VAEï¼ˆå¯é€‰ï¼‰

è¾“å‡ºï¼šè®­ç»ƒå¥½çš„å‚æ•° Î¸

1. åˆå§‹åŒ–ç½‘ç»œå‚æ•° Î¸
2. for epoch = 1 to num_epochs:
3.     for each batch {x_0^(i), c^(i)} in D:
4.         # ========== æ•°æ®å‡†å¤‡ ==========
5.         if ä½¿ç”¨ VAE:
6.             z_0 = VAE.encode(x_0)       # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
7.             z_0 = scale_factor * z_0     # ç¼©æ”¾
8.         else:
9.             z_0 = x_0

10.        # ========== å‰å‘æ‰©æ•£ ==========
11.        Ïƒ ~ p(Ïƒ)                         # é‡‡æ ·å™ªå£°çº§åˆ«ï¼ˆEDM æˆ–ç¦»æ•£ï¼‰
12.        Îµ ~ N(0, I)                      # é‡‡æ ·æ ‡å‡†é«˜æ–¯å™ªå£°
13.        z_t = z_0 + Ïƒ * Îµ                # æ·»åŠ å™ªå£°

14.        # ========== æ¡ä»¶ç¼–ç  ==========
15.        cond = conditioner(c)            # ç¼–ç æ¡ä»¶ï¼ˆæ–‡æœ¬ã€ç±»åˆ«ç­‰ï¼‰

16.        # ========== æ¨¡å‹é¢„æµ‹ï¼ˆå»å™ªå™¨ï¼‰==========
17.        c_skip, c_out, c_in, c_noise = scaling(Ïƒ)  # è®¡ç®—é¢„æ¡ä»¶ç³»æ•°

18.        # åŸå§‹ç½‘ç»œè¾“å‡º
19.        F_out = F_Î¸(z_t * c_in, c_noise, cond)

20.        # å»å™ªæ ·æœ¬é¢„æµ‹
21.        áº‘_Î¸ = c_skip * z_t + c_out * F_out

22.        # ========== è®¡ç®—æŸå¤± ==========
23.        w = loss_weighting(Ïƒ)            # è®¡ç®—æƒé‡
24.        loss = w * ||áº‘_Î¸ - z_0||Â²        # åŠ æƒ MSE

25.        # ========== åå‘ä¼ æ’­ ==========
26.        Î¸ â† Î¸ - lr * âˆ‡_Î¸ loss            # æ›´æ–°å‚ï¿½ï¿½

27. return Î¸
```

### 3.2 æ•°å­¦æµç¨‹ï¼ˆå®Œæ•´å…¬å¼ï¼‰

**Step 1: æ•°æ®å‡†å¤‡**
$$\mathbf{z}_0 = s \cdot \text{VAE}_\text{enc}(\mathbf{x}_0)$$

**Step 2: å™ªå£°çº§åˆ«é‡‡æ ·**
$$\sigma_t \sim \mathcal{LN}(p_\text{mean}, p_\text{std}^2)$$

**Step 3: å‰å‘æ‰©æ•£**
$$\mathbf{z}_t = \mathbf{z}_0 + \sigma_t \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$$

**Step 4: é¢„æ¡ä»¶ç³»æ•°è®¡ç®—**
$$c_\text{skip}(\sigma_t), c_\text{out}(\sigma_t), c_\text{in}(\sigma_t), c_\text{noise}(\sigma_t) = \text{Scaling}(\sigma_t)$$

**Step 5: ï¿½ï¿½ç»œå‰å‘ä¼ æ’­**
$$F_\theta = \text{UNet}(c_\text{in}(\sigma_t) \cdot \mathbf{z}_t, c_\text{noise}(\sigma_t), \mathbf{c})$$

**Step 6: å»å™ªæ ·æœ¬é¢„æµ‹**
$$\boxed{\hat{\mathbf{z}}_\theta = c_\text{skip}(\sigma_t) \cdot \mathbf{z}_t + c_\text{out}(\sigma_t) \cdot F_\theta}$$

**Step 7: æŸå¤±è®¡ç®—**
$$\mathcal{L}(\theta) = w(\sigma_t) \left\| \hat{\mathbf{z}}_\theta - \mathbf{z}_0 \right\|_2^2$$

**Step 8: æ¢¯åº¦ä¸‹é™**
$$\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta)$$

---

## 4. ä»£ç å®ç°è¯¦è§£

### 4.1 å®Œæ•´è®­ç»ƒæµç¨‹ä»£ç è¿½è¸ª

#### Level 1: è®­ï¿½ï¿½ï¿½å…¥å£

**æ–‡ä»¶ï¼š** `sgm/models/diffusion.py:165-187`

```python
def training_step(self, batch, batch_idx):
    """PyTorch Lightning è®­ç»ƒæ­¥éª¤"""
    # è°ƒç”¨ shared_step
    loss, loss_dict = self.shared_step(batch)

    # è®°å½•æ—¥å¿—
    self.log_dict(loss_dict, prog_bar=True, logger=True, ...)

    return loss  # è¿”å›æŸå¤±ç”¨äºè‡ªåŠ¨åå‘ä¼ æ’­
```

#### Level 2: å…±äº«æ­¥éª¤ï¼ˆæ•°æ®å‡†å¤‡ï¼‰

**æ–‡ä»¶ï¼š** `sgm/models/diffusion.py:158-163`

```python
def shared_step(self, batch: Dict) -> Any:
    """è®­ç»ƒå’ŒéªŒè¯çš„å…±äº«æ­¥éª¤"""
    # 1. è·å–å›¾åƒæ•°æ®
    x = self.get_input(batch)           # batch["jpg"]

    # 2. ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼ˆå¦‚æœä½¿ç”¨ VAEï¼‰
    x = self.encode_first_stage(x)      # VAE ç¼–ç  + ç¼©æ”¾

    # 3. æ·»åŠ å…¨å±€æ­¥æ•°åˆ° batch
    batch["global_step"] = self.global_step

    # 4. è°ƒç”¨å‰å‘ä¼ æ’­ï¼ˆè®¡ç®—æŸå¤±ï¼‰
    loss, loss_dict = self(x, batch)    # è°ƒç”¨ forward

    return loss, loss_dict
```

**VAE ç¼–ç ï¼š** `sgm/models/diffusion.py:138-150`

```python
def encode_first_stage(self, x):
    """ç¼–ç å›¾åƒåˆ°æ½œåœ¨ç©ºé—´"""
    with torch.autocast("cuda", enabled=not self.disable_first_stage_autocast):
        z = self.first_stage_model.encode(x)  # VAE ç¼–ç 

    z = self.scale_factor * z                  # ç¼©æ”¾ï¼ˆé€šå¸¸ 0.13025ï¼‰
    return z
```

#### Level 3: å‰å‘ä¼ æ’­ï¼ˆæŸå¤±è®¡ç®—ï¼‰

**æ–‡ä»¶ï¼š** `sgm/models/diffusion.py:152-156`

```python
def forward(self, x, batch):
    """è®¡ç®—æŸå¤±"""
    # è°ƒç”¨æŸå¤±å‡½æ•°
    loss = self.loss_fn(
        self.model,        # åŸå§‹ç½‘ç»œï¼ˆUNetï¼Œè¢« wrapper åŒ…è£…ï¼‰
        self.denoiser,     # å»å™ªå™¨ï¼ˆå¸¦é¢„æ¡ä»¶ï¼‰
        self.conditioner,  # æ¡ä»¶å™¨
        x,                 # æ½œåœ¨ç©ºé—´æ•°æ® z_0
        batch              # åŒ…å«æ¡ä»¶ä¿¡æ¯çš„ batch
    )

    loss_mean = loss.mean()
    loss_dict = {"loss": loss_mean}
    return loss_mean, loss_dict
```

#### Level 4: æŸå¤±å‡½æ•°ï¼ˆæ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼‰

**æ–‡ä»¶ï¼š** `sgm/modules/diffusionmodules/loss.py:48-57`

```python
def forward(
    self,
    network: nn.Module,        # UNetï¼ˆè¢« OpenAIWrapper åŒ…è£…ï¼‰
    denoiser: Denoiser,        # å»å™ªå™¨
    conditioner: GeneralConditioner,  # æ¡ä»¶å™¨
    input: torch.Tensor,       # z_0ï¼ˆæ½œåœ¨ç©ºé—´ï¼Œå¹²å‡€æ•°æ®ï¼‰
    batch: Dict,               # åŒ…å«æ¡ä»¶çš„ batch
) -> torch.Tensor:
    # 1. ç¼–ç æ¡ä»¶
    cond = conditioner(batch)

    # 2. è°ƒç”¨å†…éƒ¨å‰å‘ä¼ æ’­
    return self._forward(network, denoiser, cond, input, batch)
```

#### Level 5: æŸå¤±å‡½æ•°å†…éƒ¨å®ç°ï¼ˆå…³é”®ï¼ï¼‰

**æ–‡ä»¶ï¼š** `sgm/modules/diffusionmodules/loss.py:59-90`

```python
def _forward(
    self,
    network: nn.Module,
    denoiser: Denoiser,
    cond: Dict,
    input: torch.Tensor,       # z_0ï¼ˆç›®æ ‡ï¼‰
    batch: Dict,
) -> Tuple[torch.Tensor, Dict]:
    # ========== 1. é‡‡æ ·å™ªå£°çº§åˆ« ==========
    sigmas = self.sigma_sampler(input.shape[0]).to(input)
    # sigmas: [B]ï¼Œæ¯ä¸ªæ ·æœ¬ä¸€ä¸ª Ïƒ_t

    # ========== 2. ç”Ÿæˆå™ªå£° ==========
    noise = torch.randn_like(input)  # Îµ ~ N(0, I)

    # ï¼ˆå¯é€‰ï¼‰Offset å™ªå£°ï¼ˆæé«˜å¤šæ ·æ€§ï¼‰
    if self.offset_noise_level > 0.0:
        noise = noise + self.offset_noise_level * ...

    # ========== 3. æ·»åŠ å™ªå£°ï¼ˆå‰å‘æ‰©æ•£ï¼‰==========
    sigmas_bc = append_dims(sigmas, input.ndim)  # [B, 1, 1, 1]
    noised_input = input + noise * sigmas_bc     # z_t = z_0 + Ïƒ_t * Îµ

    # ========== 4. æ¨¡å‹é¢„æµ‹ï¼ˆå»å™ªå™¨ï¼‰==========
    model_output = denoiser(
        network,           # UNet
        noised_input,      # z_t
        sigmas,            # Ïƒ_t
        cond,              # æ¡ä»¶
        **additional_model_inputs
    )
    # model_output: áº‘_Î¸ï¼ˆé¢„æµ‹çš„å»å™ªæ ·æœ¬ï¼‰

    # ========== 5. è®¡ç®—æŸå¤± ==========
    w = append_dims(self.loss_weighting(sigmas), input.ndim)
    # w: æƒé‡ï¼Œå½¢çŠ¶ [B, 1, 1, 1]

    return self.get_loss(model_output, input, w)
    # è¿”å›ï¼šw * ||model_output - input||Â²
```

#### Level 6: å»å™ªå™¨ï¼ˆé¢„æ¡ä»¶ï¼‰

**æ–‡ä»¶ï¼š** `sgm/modules/diffusionmodules/denoiser.py:23-39`

```python
def forward(
    self,
    network: nn.Module,    # UNet
    input: torch.Tensor,   # z_tï¼ˆå™ªå£°è¾“å…¥ï¼‰
    sigma: torch.Tensor,   # Ïƒ_tï¼ˆå™ªå£°çº§åˆ«ï¼‰
    cond: Dict,            # æ¡ä»¶
    **additional_model_inputs,
) -> torch.Tensor:
    # ========== 1. æ‰©å±• sigma ç»´åº¦ ==========
    sigma = self.possibly_quantize_sigma(sigma)  # [B]
    sigma_shape = sigma.shape
    sigma = append_dims(sigma, input.ndim)       # [B, 1, 1, 1]

    # ========== 2. è®¡ç®—é¢„æ¡ä»¶ç³»æ•° ==========
    c_skip, c_out, c_in, c_noise = self.scaling(sigma)
    # æ ¹æ®ä¸åŒçš„ scaling æ–¹æ¡ˆï¼ˆEpsScaling, VScaling, EDMScalingï¼‰

    # ========== 3. è°ƒç”¨ç½‘ç»œ ==========
    c_noise = self.possibly_quantize_c_noise(c_noise.reshape(sigma_shape))
    network_output = network(
        input * c_in,      # é¢„æ¡ä»¶è¾“å…¥
        c_noise,           # å™ªå£°çº§åˆ«ï¼ˆä¼ ç»™ç½‘ç»œçš„ timestepsï¼‰
        cond,              # æ¡ä»¶
        **additional_model_inputs
    )

    # ========== 4. åº”ç”¨é¢„æ¡ä»¶å¾—åˆ°å»å™ªæ ·æœ¬ ==========
    return network_output * c_out + input * c_skip
    # áº‘_Î¸ = c_out * F_Î¸ + c_skip * z_t
```

#### Level 7: ç½‘ç»œï¼ˆUNetï¼‰

**æ–‡ä»¶ï¼š** `sgm/modules/diffusionmodules/openaimodel.py:866-903`

```python
def forward(
    self,
    x: torch.Tensor,         # input * c_in
    timesteps: torch.Tensor, # c_noiseï¼ˆå™ªå£°çº§åˆ«ï¼‰
    context: torch.Tensor = None,  # äº¤å‰æ³¨æ„åŠ›æ¡ä»¶
    y: torch.Tensor = None,        # å‘é‡æ¡ä»¶
    **kwargs,
) -> torch.Tensor:
    """UNet å‰å‘ä¼ æ’­"""
    # 1. æ—¶é—´åµŒå…¥
    t_emb = timestep_embedding(timesteps, self.model_channels)
    emb = self.time_embed(t_emb)

    # 2. ç±»åˆ«åµŒå…¥ï¼ˆå¦‚æœæœ‰ï¼‰
    if self.num_classes is not None:
        emb = emb + self.label_emb(y)

    # 3. ç¼–ç å™¨
    h = x
    hs = []
    for module in self.input_blocks:
        h = module(h, emb, context)
        hs.append(h)

    # 4. ä¸­é—´å±‚
    h = self.middle_block(h, emb, context)

    # 5. è§£ç å™¨
    for module in self.output_blocks:
        h = torch.cat([h, hs.pop()], dim=1)  # è·³è·ƒè¿æ¥
        h = module(h, emb, context)

    # 6. è¾“å‡º
    return self.out(h)  # F_Î¸
```

#### Level 8: æŸå¤±è®¡ç®—

**æ–‡ä»¶ï¼š** `sgm/modules/diffusionmodules/loss.py:92-96`

```python
def get_loss(self, model_output, target, w):
    """è®¡ç®—åŠ æƒ MSE æŸå¤±"""
    if self.loss_type == "l2":
        return torch.mean(
            (w * (model_output - target) ** 2).reshape(target.shape[0], -1),
            dim=1
        )
    elif self.loss_type == "l1":
        return torch.mean(
            (w * (model_output - target).abs()).reshape(target.shape[0], -1),
            dim=1
        )
    # ...
```

### 4.2 æ•°æ®æµå›¾è§£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½ï¿½â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è®­ç»ƒæ•°æ® batch                          â”‚
â”‚  {"jpg": images, "txt": texts, ...}                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  shared_step (diffusion.py:158)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ 1. x = batch["jpg"]                    â”‚                 â”‚
â”‚  â”‚ 2. z_0 = VAE.encode(x) * scale_factor  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ z_0ï¼ˆæ½œåœ¨ç©ºé—´ï¼Œå¹²å‡€æ•°æ®ï¼‰
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  forward (diffusion.py:152)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ loss = loss_fn(model, denoiser,       â”‚                 â”‚
â”‚  â”‚               conditioner, z_0, batch) â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  StandardDiffusionLoss._forward (loss.py:59)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ 1. Ïƒ ~ p(Ïƒ)              [é‡‡æ ·]       â”‚                 â”‚
â”‚  â”‚ 2. Îµ ~ N(0, I)           [é‡‡æ ·]       â”‚                 â”‚
â”‚  â”‚ 3. z_t = z_0 + Ïƒ * Îµ     [åŠ å™ª]       â”‚                 â”‚
â”‚  â”‚ 4. cond = conditioner(batch) [æ¡ä»¶]   â”‚                 â”‚
â”‚  â”‚ 5. áº‘_Î¸ = denoiser(...)   [é¢„æµ‹]       â”‚ â† å…³é”®ï¼
â”‚  â”‚ 6. loss = w * ||áº‘_Î¸ - z_0||Â² [æŸå¤±]   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ è¿›å…¥ denoiser
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Denoiser.forward (denoiser.py:23)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ 1. c_skip, c_out, c_in, c_noise       â”‚                 â”‚
â”‚  â”‚      = scaling(Ïƒ)        [é¢„æ¡ä»¶]     â”‚                 â”‚
â”‚  â”‚ 2. F_Î¸ = network(z_t * c_in,          â”‚                 â”‚
â”‚  â”‚                  c_noise, cond)        â”‚ â† UNet è¾“å‡º
â”‚  â”‚ 3. áº‘_Î¸ = c_skip * z_t + c_out * F_Î¸   â”‚ â† å»å™ªæ ·æœ¬
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ F_Î¸ï¼ˆç½‘ç»œè¾“å‡ºï¼‰
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNetModel.forward (openaimodel.py:866)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ è¾“å…¥ï¼šz_t * c_in, c_noise, cond       â”‚                 â”‚
â”‚  â”‚ è¾“å‡ºï¼šF_Î¸ï¼ˆæ®‹å·®/å™ªå£°/é€Ÿåº¦é¢„æµ‹ï¼‰      â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. å¯æ›¿æ¢ç‚¹åˆ†æ

### 5.1 å…³é”®æ›¿æ¢ä½ç½®

#### â­ **æ›¿æ¢ç‚¹ 1ï¼šDenoiser è¾“å‡ºï¼ˆæ¨èï¼‰**

**ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser.py:36-39`

**å½“å‰å®ç°ï¼š**
```python
return (
    network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out
    + input * c_skip
)
# è¾“å‡ºï¼šå»å™ªæ ·æœ¬ áº‘_Î¸
```

**æ›¿æ¢ä¸ºåˆ†æ•°å‡½æ•°ï¼š**

å¦‚æœä½ çš„æ¨¡å‹è¾“å‡ºåˆ†æ•° $\mathbf{s}_\theta$ï¼Œåœ¨è¿™é‡Œæ·»åŠ è½¬æ¢ï¼š

```python
# å½“å‰ä»£ç ï¼ˆå»å™ªæ ·æœ¬ï¼‰
# áº‘_Î¸ = c_skip * z_t + c_out * F_Î¸

# å¦‚æœ F_Î¸ è¾“å‡ºåˆ†æ•°ï¼Œéœ€è¦è½¬æ¢
network_output = network(input * c_in, c_noise, cond, **additional_model_inputs)

# åˆ¤æ–­æ˜¯å¦æ˜¯åˆ†æ•°è¾“å‡º
if self.is_score_output:  # æ–°å¢æ ‡å¿—
    # è½¬æ¢ï¼šáº‘_Î¸ = z_t + ÏƒÂ² * score
    score = network_output
    sigma = append_dims(sigma, input.ndim)  # ç¡®ä¿ sigma å·²æ‰©å±•
    denoised = input + (sigma ** 2) * score
    return denoised
else:
    # åŸå§‹é€»è¾‘
    return network_output * c_out + input * c_skip
```

**æ•°å­¦å…¬å¼ï¼š**

**åŸå§‹ï¼ˆå»å™ªæ ·æœ¬ï¼‰ï¼š**
$$\hat{\mathbf{z}}_\theta = c_\text{skip}(\sigma_t) \cdot \mathbf{z}_t + c_\text{out}(\sigma_t) \cdot F_\theta$$

**æ›¿æ¢ï¼ˆåˆ†æ•°å‡½æ•°ï¼‰ï¼š**
$$\boxed{\hat{\mathbf{z}}_\theta = \mathbf{z}_t + \sigma_t^2 \cdot \mathbf{s}_\theta}$$

å…¶ä¸­ $\mathbf{s}_\theta = F_\theta(\mathbf{z}_t, \sigma_t, \mathbf{c})$ æ˜¯ä½ çš„æ¨¡å‹è¾“å‡ºã€‚

---

#### â­ **æ›¿æ¢ç‚¹ 2ï¼šè‡ªå®šä¹‰ Scaling ç±»**

**ä½ç½®ï¼š** `sgm/modules/diffusionmodules/denoiser_scaling.py`

**åˆ›å»ºæ–°çš„ ScoreScaling ç±»ï¼š**

```python
class ScoreScaling:
    """
    ç”¨äºè¾“å‡ºåˆ†æ•°å‡½æ•°çš„æ¨¡å‹

    ç½‘ç»œè¾“å‡ºï¼šscore = âˆ‡log p(z_t)
    å»å™ªæ ·æœ¬ï¼šáº‘_Î¸ = z_t + ÏƒÂ² * score

    å»å™ªå™¨å…¬å¼ï¼šáº‘_Î¸ = c_skip * z_t + c_out * network_output
    å› æ­¤ï¼šc_skip = 1, c_out = ÏƒÂ²
    """
    def __call__(
        self, sigma: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        c_skip = torch.ones_like(sigma, device=sigma.device)  # 1
        c_out = sigma ** 2                                     # ÏƒÂ²
        c_in = torch.ones_like(sigma, device=sigma.device)    # 1
        c_noise = sigma.clone()                                # Ïƒ
        return c_skip, c_out, c_in, c_noise
```

**é…ç½®æ–‡ä»¶ä½¿ç”¨ï¼š**
```yaml
denoiser_config:
  target: sgm.modules.diffusionmodules.denoiser.Denoiser
  params:
    scaling_config:
      target: sgm.modules.diffusionmodules.denoiser_scaling.ScoreScaling
```

**æ•°å­¦éªŒè¯ï¼š**
$$\begin{aligned}
\hat{\mathbf{z}}_\theta &= c_\text{skip} \cdot \mathbf{z}_t + c_\text{out} \cdot \mathbf{s}_\theta \\
&= 1 \cdot \mathbf{z}_t + \sigma_t^2 \cdot \mathbf{s}_\theta \\
&= \mathbf{z}_t + \sigma_t^2 \cdot \mathbf{s}_\theta \quad \checkmark
\end{aligned}$$

---

#### â­ **æ›¿æ¢ç‚¹ 3ï¼šNetwork åŒ…è£…å™¨**

**ä½ç½®ï¼š** `sgm/modules/diffusionmodules/wrappers.py` æˆ–åˆ›å»ºæ–°æ–‡ä»¶

**åˆ›å»º ScoreToDenoised åŒ…è£…å™¨ï¼š**

```python
class ScoreToDenoised(nn.Module):
    """å°†åˆ†æ•°æ¨¡å‹åŒ…è£…ä¸ºå»å™ªæ¨¡å‹"""

    def __init__(self, score_model: nn.Module):
        super().__init__()
        self.score_model = score_model

    def forward(
        self,
        x: torch.Tensor,        # z_t * c_in
        timesteps: torch.Tensor,  # c_noiseï¼ˆÏƒ æˆ–å…¶å˜æ¢ï¼‰
        context: torch.Tensor = None,
        y: torch.Tensor = None,
        **kwargs
    ) -> torch.Tensor:
        # 1. è°ƒç”¨åˆ†æ•°æ¨¡å‹
        score = self.score_model(x, timesteps, context, y, **kwargs)

        # 2. è¿™é‡Œä¸éœ€è¦è½¬æ¢ï¼
        # å› ä¸º denoiser ä¼šå¤„ç†ï¼šáº‘ = c_skip * z_t + c_out * score
        # åªè¦é…ç½®æ­£ç¡®çš„ scalingï¼ˆc_out = ÏƒÂ²ï¼‰ï¼Œå°±èƒ½å¾—åˆ°æ­£ç¡®ç»“æœ

        return score  # ç›´æ¥è¿”å›åˆ†æ•°
```

**é…ç½®ä½¿ç”¨ï¼š**
```yaml
network_config:
  target: sgm.modules.diffusionmodules.wrappers.OpenAIWrapper
  params:
    diffusion_model:
      target: path.to.ScoreToDenoised
      params:
        score_model:
          target: your_module.YourScoreModel
          params:
            # ä½ çš„æ¨¡å‹å‚æ•°
```

---

### 5.2 ä¸‰ç§æ›¿æ¢æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¿®æ”¹ä½ç½® | å¤æ‚åº¦ | çµæ´»æ€§ | æ¨èåº¦ |
|------|---------|--------|--------|--------|
| **æ–¹æ¡ˆ 1ï¼šä¿®æ”¹ Denoiser** | `denoiser.py:36-39` | ä½ | ä¸­ | â­â­â­ |
| **æ–¹æ¡ˆ 2ï¼šè‡ªå®šä¹‰ Scaling** | `denoiser_scaling.py` | ä½ | é«˜ | â­â­â­â­â­ |
| **æ–¹æ¡ˆ 3ï¼šNetwork åŒ…è£…å™¨** | åˆ›å»ºæ–°æ–‡ä»¶ | ä¸­ | é«˜ | â­â­â­â­ |

**æ¨èï¼šæ–¹æ¡ˆ 2ï¼ˆè‡ªå®šä¹‰ Scalingï¼‰** - æœ€å¹²å‡€ï¼Œæœ€ç¬¦åˆæ¡†æ¶è®¾è®¡

---

### 5.3 å®Œæ•´æ›¿æ¢ç¤ºä¾‹

#### å‡è®¾ä½ çš„æ¨¡å‹

```python
class YourScoreModel(nn.Module):
    """ä½ çš„åˆ†æ•°å‡½æ•°æ¨¡å‹"""

    def forward(
        self,
        x: torch.Tensor,           # z_tï¼ˆæˆ– z_t * c_inï¼‰
        timesteps: torch.Tensor,   # Ïƒ_tï¼ˆæˆ–å…¶å˜æ¢ c_noiseï¼‰
        context: torch.Tensor = None,
        y: torch.Tensor = None,
        **kwargs
    ) -> torch.Tensor:
        """
        è¾“å‡ºï¼šåˆ†æ•°å‡½æ•° âˆ‡log p(z_t)
        """
        # ä½ çš„å®ç°
        # ...
        return score  # [B, C, H, W]
```

#### ä½¿ç”¨æ–¹æ¡ˆ 2ï¼ˆè‡ªå®šä¹‰ Scalingï¼‰

**Step 1: åˆ›å»º ScoreScaling**

åœ¨ `sgm/modules/diffusionmodules/denoiser_scaling.py` æ·»åŠ ï¼š

```python
class ScoreScaling:
    def __call__(self, sigma: torch.Tensor):
        c_skip = torch.ones_like(sigma)
        c_out = sigma ** 2
        c_in = torch.ones_like(sigma)
        c_noise = sigma.clone()
        return c_skip, c_out, c_in, c_noise
```

**Step 2: é…ç½®æ–‡ä»¶**

```yaml
model:
  target: sgm.models.diffusion.DiffusionEngine
  params:
    network_config:
      target: sgm.modules.diffusionmodules.wrappers.OpenAIWrapper
      params:
        diffusion_model:
          target: your_module.YourScoreModel
          params:
            # ä½ çš„å‚æ•°

    denoiser_config:
      target: sgm.modules.diffusionmodules.denoiser.Denoiser
      params:
        scaling_config:
          target: sgm.modules.diffusionmodules.denoiser_scaling.ScoreScaling

    # ... å…¶ä»–é…ç½®
```

**Step 3: è®­ç»ƒ**

```bash
python main.py --base configs/your_config.yaml
```

**æ•°æ®æµï¼š**
```
z_tï¼ˆå™ªå£°æ•°æ®ï¼‰
    â†“
Denoiser:
    c_skip=1, c_out=ÏƒÂ², c_in=1, c_noise=Ïƒ
    â†“
YourScoreModel(z_t * 1, Ïƒ, cond) â†’ score
    â†“
áº‘_Î¸ = 1 * z_t + ÏƒÂ² * score = z_t + ÏƒÂ² * score  âœ“
    â†“
Loss = w * ||áº‘_Î¸ - z_0||Â²
```

---

### 5.4 å…³é”®ç‚¹æ€»ç»“

#### âœ… å¯ä»¥æ›¿æ¢çš„åœ°æ–¹

1. **Denoiser.forward**ï¼ˆç›´æ¥ä¿®æ”¹è¿”å›å€¼ï¼‰
2. **Denoiser Scaling**ï¼ˆä¿®æ”¹é¢„æ¡ä»¶ç³»æ•°ï¼Œæ¨èï¼‰
3. **Network åŒ…è£…å™¨**ï¼ˆåœ¨æ¨¡å‹å¤–éƒ¨æ·»åŠ è½¬æ¢å±‚ï¼‰
4. **Loss å‡½æ•°**ï¼ˆå¦‚æœè¦æ”¹è®­ç»ƒç›®æ ‡ï¼‰

#### âŒ ä¸éœ€è¦æ›¿æ¢çš„åœ°æ–¹

1. **å‰å‘æ‰©æ•£**ï¼ˆ`z_t = z_0 + Ïƒ * Îµ`ï¼‰- ä¿æŒä¸å˜
2. **Sigma é‡‡æ ·**ï¼ˆEDMSampling ç­‰ï¼‰- ä¿æŒä¸å˜
3. **æ¡ä»¶å™¨**ï¼ˆConditionerï¼‰- ä¿æŒä¸å˜
4. **VAE ç¼–ç /è§£ç **- ä¿æŒä¸å˜
5. **æŸå¤±è®¡ç®—**ï¼ˆMSEï¼‰- ä¿æŒä¸å˜

#### ğŸ”‘ æ ¸å¿ƒæ•°å­¦å…³ç³»

**æ— è®ºä½ çš„æ¨¡å‹è¾“å‡ºä»€ä¹ˆï¼Œæœ€ç»ˆéƒ½è¦è½¬æ¢ä¸ºå»å™ªæ ·æœ¬ï¼š**

$$\hat{\mathbf{z}}_\theta = \begin{cases}
\mathbf{z}_t - \sigma_t \boldsymbol{\epsilon}_\theta & \text{å™ªå£°é¢„æµ‹} \\
\mathbf{z}_t + \sigma_t^2 \mathbf{s}_\theta & \text{åˆ†æ•°é¢„æµ‹} \\
\text{ç›´æ¥è¾“ï¿½ï¿½} & \text{å»å™ªæ ·æœ¬é¢„æµ‹}
\end{cases}$$

**è®­ç»ƒç›®æ ‡å§‹ç»ˆæ˜¯ï¼š**
$$\min_\theta \mathbb{E} \left[ w(\sigma_t) \left\| \hat{\mathbf{z}}_\theta - \mathbf{z}_0 \right\|^2 \right]$$

---

## 6. æ€»ç»“

### 6.1 è®­ç»ƒæµç¨‹å…¬å¼æ€»ç»“

$$\begin{aligned}
\text{æ•°æ®ï¼š} & \quad \mathbf{z}_0 = s \cdot \text{VAE}_\text{enc}(\mathbf{x}_0) \\
\text{é‡‡æ ·ï¼š} & \quad \sigma_t \sim p(\sigma), \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}) \\
\text{åŠ å™ªï¼š} & \quad \mathbf{z}_t = \mathbf{z}_0 + \sigma_t \boldsymbol{\epsilon} \\
\text{æ¡ä»¶ï¼š} & \quad \mathbf{c} = \text{Conditioner}(\text{batch}) \\
\text{é¢„æµ‹ï¼š} & \quad \hat{\mathbf{z}}_\theta = c_\text{skip}(\sigma_t) \mathbf{z}_t + c_\text{out}(\sigma_t) F_\theta(c_\text{in}(\sigma_t) \mathbf{z}_t, c_\text{noise}(\sigma_t), \mathbf{c}) \\
\text{æŸå¤±ï¼š} & \quad \mathcal{L} = w(\sigma_t) \|\hat{\mathbf{z}}_\theta - \mathbf{z}_0\|^2
\end{aligned}$$

### 6.2 åˆ†æ•°å‡½æ•°æ›¿æ¢è¦ç‚¹

**å¦‚æœä½ çš„æ¨¡å‹è¾“å‡ºåˆ†æ•° $\mathbf{s}_\theta$ï¼š**

1. **è½¬æ¢å…¬å¼ï¼š** $\hat{\mathbf{z}}_\theta = \mathbf{z}_t + \sigma_t^2 \mathbf{s}_\theta$

2. **å®ç°æ–¹å¼ï¼š** ä½¿ç”¨ `ScoreScaling`ï¼ˆ$c_\text{skip}=1, c_\text{out}=\sigma^2$ï¼‰

3. **æ— éœ€ä¿®æ”¹ï¼š** æŸå¤±å‡½æ•°ã€å‰å‘æ‰©æ•£ã€é‡‡æ ·å™¨ç­‰ä¿æŒä¸å˜

4. **sigma å¯è·å–ï¼š** åœ¨æ‰€æœ‰åœ°æ–¹éƒ½å¯ä»¥è®¿é—® $\sigma_t$

### 6.3 ä»£ç å…³é”®ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œå· | è¯´æ˜ |
|-----|------|-----|------|
| è®­ç»ƒå…¥å£ | `diffusion.py` | 165-187 | training_step |
| æ•°æ®å‡†å¤‡ | `diffusion.py` | 158-163 | VAE ç¼–ç  |
| æŸå¤±è®¡ç®— | `loss.py` | 59-90 | å‰å‘æ‰©æ•£ + é¢„æµ‹ |
| å»å™ªå™¨ | `denoiser.py` | 23-39 | é¢„æ¡ä»¶ + ç½‘ç»œè°ƒç”¨ |
| é¢„æ¡ä»¶ | `denoiser_scaling.py` | å…¨æ–‡ | c_skip, c_out, c_in, c_noise |
| UNet | `openaimodel.py` | 866-903 | ç½‘ç»œå‰å‘ä¼ æ’­ |

**è¿™å°±æ˜¯å®Œæ•´çš„è®­ç»ƒæµç¨‹åˆ†æï¼**
