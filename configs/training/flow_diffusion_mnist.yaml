# Flow-based Diffusion Model Configuration for MNIST
# Uses Normalizing Flow instead of UNet for score computation

model:
  base_learning_rate: 1.0e-4
  target: sgm.models.score_flow.FlowDiffusionEngine
  params:
    input_key: jpg
    scale_factor: 1.0

    score_network_config:
      target: sgm.models.score_flow.ScoreFlowNetwork
      params:
        data_dim: 784  # 28*28 for MNIST
        n_flows: 3
        activation: softplus
        activation_params:
          beta: 1.0
        sigma_embed_dim: 256
        cond_embed_dim: 128
        use_conditioning: true

    conditioner_config:
      target: sgm.modules.GeneralConditioner
      params:
        emb_models:
          - is_trainable: true
            input_key: cls
            ucg_rate: 0.1
            target: sgm.modules.encoders.modules.ClassEmbedder
            params:
              embed_dim: 128
              n_classes: 10

    first_stage_config:
      target: sgm.models.autoencoder.IdentityFirstStage

    loss_fn_config:
      target: sgm.modules.diffusionmodules.loss.StandardDiffusionLoss
      params:
        loss_weighting_config:
          target: sgm.modules.diffusionmodules.loss_weighting.EDMWeighting
          params:
            sigma_data: 0.5
        sigma_sampler_config:
          target: sgm.modules.diffusionmodules.sigma_sampling.EDMSampling
          params:
            p_mean: -1.2
            p_std: 1.2

    optimizer_config:
      target: torch.optim.AdamW
      params:
        lr: 1.0e-4
        betas: [0.9, 0.999]
        weight_decay: 0.01

data:
  target: sgm.data.mnist.MNISTLoader
  params:
    batch_size: 256
    num_workers: 4

lightning:
  modelcheckpoint:
    params:
      every_n_train_steps: 5000
      save_top_k: 3
      monitor: train/loss

  callbacks:
    image_logger:
      target: main.ImageLogger
      params:
        disabled: false
        batch_frequency: 1000
        max_images: 16
        increase_log_steps: true
        log_first_step: false
        log_images_kwargs:
          N: 16
          n_rows: 4

  trainer:
    devices: 0,
    benchmark: true
    num_sanity_val_steps: 0
    accumulate_grad_batches: 1
    max_epochs: 50
    gradient_clip_val: 1.0
